# LoRA Finetuning: RoBERTa-Base on Few-NERD (Supervised)

---

Live DEMO - deployed here on [HF spaces](https://huggingface.co/spaces/ajnx014/LoRA-finetuned-NER)

---

This section documents the full finetuning pipeline used to train a parameter-efficient NER model using **LoRA adapters** on top of **RoBERTa-base**, optimized for deployment on low-memory environments (AWS EC2, Lambda, containers). if you wish to jump to AWS deployment part of the markdown, click here - 
AWS deployment Setup - [click here](https://github.com/ArjunJagdale/NER/blob/main/README.md#-aws-deployment-fastapi--docker--ec2)

## What is Named Entity Recognition (NER)?

Named Entity Recognition (NER) is a core Natural Language Processing (NLP) task where a model identifies and classifies meaningful pieces of text (called entities) into predefined categories.

In simple terms, NER turns unstructured text into structured information by answering two questions:

What is the entity? (e.g., ‚ÄúBarack Obama‚Äù)

What type is it? (e.g., person-politician)

---

## üìä Dataset Configuration

We use the **Few-NERD (supervised)** setting with fine-grained entity types. A larger dataset is sampled for LoRA efficiency:

| Split      | Samples Used | % of Original        |
| ---------- | ------------ | -------------------- |
| Train      | **100,000**  | 75.9%                |
| Validation | **15,000**   | 79.7%                |
| Test       | **37,648**   | 100% (full test set) |

**Total training samples:** 115,000
**Expected training time:** ~20‚Äì30 minutes (A100 / T4 with LoRA)

---

## ‚öôÔ∏è LoRA Configuration

| Parameter      | Value                |
| -------------- | -------------------- |
| Task           | Token Classification |
| Rank (r)       | **16**               |
| Alpha          | **32**               |
| Dropout        | **0.1**              |
| Target Modules | `query`, `value`     |
| Bias           | none                 |

---

## üßÆ Parameter Efficiency

| Metric                  | Value                |
| ----------------------- | -------------------- |
| Total model parameters  | **124,747,910**      |
| Trainable params (LoRA) | **641,347**          |
| Trainable %             | **0.51%**            |
| Frozen params           | 124,106,563 (99.49%) |

**Parameter efficiency:** 194.5√ó fewer trainable parameters
**Memory savings:** ~99.5% reduction in trainable memory footprint

---

## üèãÔ∏è Training Configuration

| Setting               | Value    |
| --------------------- | -------- |
| Epochs                | **5**    |
| Batch size            | 16       |
| Gradient accumulation | 2        |
| Effective batch size  | **32**   |
| Learning rate         | **3e-4** |
| Warmup ratio          | 0.1      |
| Mixed precision       | FP16     |
| Total training steps  | ~15,625  |

---

## üìà Validation Metrics (per Epoch)

| Epoch | Train Loss | Val Loss | Precision | Recall | F1         |
| ----- | ---------- | -------- | --------- | ------ | ---------- |
| 1     | 0.2823     | 0.2627   | 0.6284    | 0.6739 | 0.6503     |
| 2     | 0.2627     | 0.2478   | 0.6488    | 0.6843 | 0.6661     |
| 3     | 0.2464     | 0.2450   | 0.6449    | 0.6936 | 0.6683     |
| 4     | 0.2350     | 0.2390   | 0.6617    | 0.6940 | 0.6774     |
| 5     | 0.2303     | 0.2380   | 0.6607    | 0.7003 | **0.6800** |

**Best validation F1:** 0.6800

---

## üß™ Test Set Results

| Metric         | Score      |
| -------------- | ---------- |
| **Test F1**    | **0.6744** |
| Test Precision | 0.6539     |
| Test Recall    | 0.6961     |
| Test Loss      | 0.2431     |

---

## üì¶ Model Artifacts

### **LoRA Adapters MERGED TO BASE MODEL**

* Path: `./roberta-lora-fewnerd-merged`
* Loads like a standard Transformers model:

  ```python
  AutoModelForTokenClassification.from_pretrained("roberta-lora-fewnerd-merged")
  ```

---
---

# üöÄ AWS Deployment (FastAPI + Docker + EC2)

This project includes a full production-style deployment of the fine-tuned NER model to **AWS EC2** using **Docker** and **FastAPI**.
The entire pipeline is lightweight, reproducible, and works on low-cost instances such as **t3.micro**.

---

## üèóÔ∏è 1. Infrastructure Overview

**Service:** AWS EC2
**Instance Type:** `t3.micro` (2 vCPUs, 1GB RAM)
**AMI:** Ubuntu 22.04 LTS
**Model Size:** ~300MB (LoRA merged RoBERTa model)
**Serving Stack:**

* FastAPI
* Uvicorn
* Docker
* CPU-only PyTorch
* Custom NERPredictor class (Hugging Face Transformers)

Instance Config
<img width="1916" height="820" alt="AWS1" src="https://github.com/user-attachments/assets/f3b33909-ea08-4153-9685-f52733b01c0b" />

Instance Config
<img width="1917" height="823" alt="AWS" src="https://github.com/user-attachments/assets/ae756780-dba6-400f-b648-ecfd5be9298c" />


Instance Config
<img width="1915" height="824" alt="image" src="https://github.com/user-attachments/assets/6a7efc7e-e741-4048-af98-7633906853cb" />



---

## üß± 2. Folder Structure Shipped to EC2

The entire app folder is packaged into a `tar.gz` archive and uploaded:

```
lora-ner-full.tar.gz
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îî‚îÄ‚îÄ predictor.py
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ roberta-lora-fewnerd-merged/
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ .dockerignore
```

This ensures a clean, deterministic environment for Docker.

---

# ‚öôÔ∏è 3. FastAPI Application (app/main.py)

The API exposes three endpoints:

* `/` ‚Üí API health + model info
* `/health` ‚Üí for container health checks
* `/predict` ‚Üí NER inference endpoint

Features:

* Proper error handling
* Logging
* Pydantic validation
* Model loaded once at startup
* CPU-optimized inference path

---

# üß† 4. NERPredictor (app/predictor.py)

A minimal inference wrapper that handles:

* Tokenization
* Model forwarding
* Argmax decoding
* Entity reconstruction
* Device management (CPU/GPU)

It works with any `AutoModelForTokenClassification` checkpoints.

---

# üì¶ 5. Dockerfile (Deployable Container Image)

Dockerfile:

* Installs Python deps
* Copies model + app into `/app`
* Runs Uvicorn at `0.0.0.0:8000`
* Adds AWS-compatible health checks
* Disables parallel tokenizers (fixes crashes)

This ensures the container is production-ready and works even on minimal CPU instances.

---

# üñ•Ô∏è 6. EC2 Deployment Steps

### **1Ô∏è‚É£ Create EC2 instance**

* AMI: Ubuntu 22.04
* Instance: `t3.micro`
* Open inbound rules:

  * `22` (SSH)
  * `8000` (API)

### **2Ô∏è‚É£ Upload project**

```
sftp -i lora-ner-key.pem ubuntu@<EC2_IP>
put lora-ner-full.tar.gz
```

### **3Ô∏è‚É£ SSH into EC2**

```
ssh -i lora-ner-key.pem ubuntu@<EC2_IP>
```

### **4Ô∏è‚É£ Extract project**

```
mkdir -p ~/lora-ner
tar -xzf lora-ner-full.tar.gz -C ~/lora-ner/
cd ~/lora-ner
```

### **5Ô∏è‚É£ Build Docker image**

```
docker build -t ner-api:v1 .
```

### **6Ô∏è‚É£ Run container**

```
docker run -d --name ner-api -p 8000:8000 --restart unless-stopped ner-api:v1
```

### **7Ô∏è‚É£ Check container health**

```
docker ps
docker logs ner-api
```

### **8Ô∏è‚É£ Test API on EC2**

```
curl http://localhost:8000
curl http://localhost:8000/predict -d '{"text":"Barack Obama was president of USA"}'
```

### **9Ô∏è‚É£ Test from outside**

```
curl http://<EC2_PUBLIC_IP>:8000
curl http://<EC2_PUBLIC_IP>:8000/predict ...
```

Everything becomes publicly accessible at:

```
http://<YOUR_EC2_PUBLIC_IP>:8000/docs
```

---

## Local ENDPOINT TESTING - 

<img width="1336" height="717" alt="Screenshot 2025-11-26 194519" src="https://github.com/user-attachments/assets/82b737b5-0fa1-48f6-acc2-1af243cd6677" />

<img width="1339" height="200" alt="Screenshot 2025-11-26 194533" src="https://github.com/user-attachments/assets/8b1e5591-11e0-423c-bcf0-bc0afd33fe03" />

